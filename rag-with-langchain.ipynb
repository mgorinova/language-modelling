{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V51p_Ia0zhtM"
      ],
      "mount_file_id": "1tIFkCicJOJuHWuFG_It4BZawcWGxtt9O",
      "authorship_tag": "ABX9TyOFuTHGe+RpLetu8XpTuXrx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgorinova/language-modelling/blob/main/rag-with-langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prelims"
      ],
      "metadata": {
        "id": "V51p_Ia0zhtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchainhub sentence_transformers\n",
        "\n",
        "!pip install pypdf pdfminer.six\n",
        "!pip install chromadb\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "iEQSYxcEDKj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "ZgeJzgu2SFst"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text wrapping of output cells, so that content is more readable."
      ],
      "metadata": {
        "id": "tmUk1Y6bzW1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "MaJI8LMEzRHe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "The goal of this Retrieval-Augmented Generation (RAG) task is to outline a prototype of a pipeline where we can ask natural language questions about the information in one of more input documents. The output should help answer the user's question, with a citation of the section of the document where this information is.\n",
        "\n",
        "This notebook uses Python and LangChain to implement such a prototype as follows:\n",
        "1. Extraction: Text is extracted from the .pdf documents and split into cunks.\n",
        "2. Embedding: The chunks are passed through a LLM to create embeddings. The embeddings are stored in a vector database (VB).\n",
        "3. Query the VB: Questions from the user and embedded with the same language model. We retrieve the most similar (in terms of embeddings) chunks from the VB.\n",
        "4. Prompt: The text of the chunks is incorporated into a LLM prompt as a context. The prompt also asks the question provided by the user.\n",
        "5. Assamble response.\n",
        "\n",
        "## 1. Extraction\n",
        "\n",
        "Use LangChain's document loaders to load the pdf files provided. The code assumes that all pdf files to be processed are in the local folder `data/`.\n",
        "\n",
        "For this example, I included two papers: the 2017 NeurIPS paper \"Attention is All You Need\", and the 2021 ICCV \"Swin Transformer\" paper."
      ],
      "metadata": {
        "id": "ZJ2cu9hFGTWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CVab0xkJChNa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fc4bdb74-2c71-4043-f9a2-fb6d33d02be3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.document_loaders.pdf import PyPDFLoader\n",
        "\n",
        "data_path = \"data/\"\n",
        "\n",
        "pages = []\n",
        "for file_name in os.listdir(data_path):\n",
        "  file_path = os.path.join(data_path, file_name)\n",
        "  loader = PyPDFLoader(file_path)\n",
        "  pages.extend(loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the text into chunks. Using appropriate separateros, shunk sizes and overlap size."
      ],
      "metadata": {
        "id": "1tsSvlIzfvLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100, add_start_index=True)\n",
        "\n",
        "splits = text_splitter.split_documents(pages)\n",
        "\n",
        "print(splits[2].metadata)\n",
        "print(splits[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Ruc8YZH3OX2A",
        "outputId": "fcbe95e7-153d-4898-f2ee-a1a3d81c0170"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': 'data/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf', 'page': 0, 'start_index': 1842}\n",
            "and models are publicly available at https://github.\n",
            "com/microsoft/Swin-Transformer .\n",
            "1. Introduction\n",
            "Modeling in computer vision has long been dominated\n",
            "by convolutional neural networks (CNNs). Beginning with\n",
            "AlexNet [35] and its revolutionary performance on the\n",
            "ImageNet image classiﬁcation challenge, CNN architec-\n",
            "*Equal contribution.†Interns at MSRA.‡Contact person.\n",
            "Figure 1. (a) The proposed Swin Transformer builds hierarchical\n",
            "feature maps by merging image patches (shown in gray) in deeper\n",
            "layers and has linear computation complexity to input image size\n",
            "due to computation of self-attention only within each local win-\n",
            "dow (shown in red). It can thus serve as a general-purpose back-\n",
            "bone for both image classiﬁcation and dense recognition tasks.\n",
            "(b) In contrast, previous vision Transformers [19] produce fea-\n",
            "ture maps of a single low resolution and have quadratic compu-\n",
            "tation complexity to input image size due to computation of self-\n",
            "attention globally.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Embeddings\n",
        "\n",
        "Embed the chunks using an LLM (in this case using HuggingFace `sentence_transformers` embedding models; with the default model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2))."
      ],
      "metadata": {
        "id": "5pCXUoJ5Sgek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "47NKKOU6GzR5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b8b36323-2080-4662-cec8-9d2bbd6d803f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Query the Vector Database\n",
        "\n",
        "Next, we assume a question from the user and query the VB based on it to retrieve the top 5 most similar chunks in terms of semantics.\n",
        "\n",
        "I prepared 3 questions for this task: one relating to Document 1 (Attention is All You Need), one relating to Document 2 (Swin), and one relating to both."
      ],
      "metadata": {
        "id": "U-M9N1N2kbeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#question = \"Can I use self-attention for natural language processing tasks?\"\n",
        "#question = \"Can I use self-attention for computer vision tasks?\"\n",
        "question = \"What is Transformer?\"\n",
        "\n",
        "retrieved_chunks = vectorstore.similarity_search(query=question, k=5)\n",
        "\n",
        "[doc.metadata for doc in retrieved_chunks]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "ckpK5W5FocB_",
        "outputId": "d6f7fd2c-c936-4e81-df7d-b3a53909a3b7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page': 2,\n",
              "  'source': 'data/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
              "  'start_index': 0},\n",
              " {'page': 4,\n",
              "  'source': 'data/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
              "  'start_index': 503},\n",
              " {'page': 1,\n",
              "  'source': 'data/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf',\n",
              "  'start_index': 0},\n",
              " {'page': 1,\n",
              "  'source': 'data/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
              "  'start_index': 3529},\n",
              " {'page': 1,\n",
              "  'source': 'data/NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
              "  'start_index': 3529}]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_chunks[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "NLU43eeJ6sGk",
        "outputId": "0408e664-1ed7-43f4-9110-cad3c0ae5071"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this paper, we seek to expand the applicability of\n",
            "Transformer such that it can serve as a general-purpose\n",
            "backbone for computer vision, as it does for NLP and\n",
            "as CNNs do in vision. We observe that signiﬁcant chal-\n",
            "lenges in transferring its high performance in the language\n",
            "domain to the visual domain can be explained by differ-\n",
            "ences between the two modalities. One of these differ-\n",
            "ences involves scale. Unlike the word tokens that serve\n",
            "as the basic elements of processing in language Trans-\n",
            "formers, visual elements can vary substantially in scale, a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prompt\n",
        "\n",
        "We create a prompt for a LLM chatbot (in this case ChatGPT 3.5). The prompt incorporates the retrieved contexts and asks the chatbot to answer the question based on those contexts, by also providing a citation --- which contexts were the most influential for the construction of the chatbot's answer."
      ],
      "metadata": {
        "id": "y48pPodzx7l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "qIp1ktyxLZwz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3b7e1dd5-6dfc-4434-efe6-b2dad48bba15"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.document import Document\n",
        "\n",
        "def format_prompt(question: str, documents: list[Document]) -> str:\n",
        "  \"\"\"Function to create the LLM prompt based on the question and list of\n",
        "  retrieved documents.\"\"\"\n",
        "\n",
        "  formatted_context = \"\"\n",
        "  for i, d in enumerate(documents):\n",
        "    formatted_context += (\n",
        "      f\"Context {i+1}, source file {d.metadata['source']}, \"\n",
        "      f\"page {d.metadata['page']}, \"\n",
        "      f\"location {d.metadata['start_index']}:\\n{d.page_content}\\n\")\n",
        "\n",
        "  return f\"\"\"You are an assistant for question-answering tasks on documents.\n",
        "Use the following pieces of retrieved context from several existing documents to\n",
        "answer the question. If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Do not ask me questions, simply answer this query. State the IDs of the 1 to 3\n",
        "contexts that provided the most significant information for answering the query.\n",
        "Do this at the end of your response, in brackets like so: [2,3]\n",
        "\n",
        "Question: {question}\n",
        "{formatted_context}\n",
        "Answer:\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nJrjvSWYJiNP",
        "outputId": "52e8afbc-cc28-46e1-a97e-ecd75a56ee92"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input = format_prompt(documents=retrieved_chunks, question=question)\n",
        "response = llm.invoke(input=llm_input)"
      ],
      "metadata": {
        "id": "SB2hg0u0eUxZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "88fee7c1-84b0-478c-efc9-3b726000bd4d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "d19Bx9D7njjF",
        "outputId": "979a27f9-39e9-46fd-a8ca-3434853108bc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer is a model architecture that employs a residual connection and layer normalization. It consists of an encoder and a decoder, both composed of stacked self-attention and fully connected layers. The Transformer is used in various domains such as NLP and computer vision. [1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract citation IDs from the response.\n",
        "\n",
        "**Note**: this has not been adapted to work in cases where citations were not returned, or the IDs were invalid.\n",
        "\n",
        "Perhaps a better way of doing this would be to embed the response and display / cite the most similar to it chunks of text, instead of relying on the chatbot to tell us."
      ],
      "metadata": {
        "id": "e951VwTvBnfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "citations = re.findall(r'\\[(([1-9]+, ?)*([1-9]+))\\].? ?$', response.content)[0][0]\n",
        "context_ids = [int(c) for c in re.split(r'( ,)|,', citations) if c is not None]\n",
        "context_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "nLQzuZZsAP_W",
        "outputId": "00258730-0889-4c6b-d01f-5443f15a899f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Assemble the response\n"
      ],
      "metadata": {
        "id": "0UCaGSigCALS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_texts = []\n",
        "for context_id in context_ids:\n",
        "  context_texts.append(retrieved_chunks[context_id-1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Dxabe0RgHzFd",
        "outputId": "96ec7551-6b82-4a1a-a494-3b79f1855135"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Question: {question}\\n\")\n",
        "\n",
        "print(f\"Response: {response.content}\\n\")\n",
        "\n",
        "for context_id, context_text in zip(context_ids, context_texts):\n",
        "  context_metadata = retrieved_chunks[context_id-1].metadata\n",
        "  print(\n",
        "      f\"[{context_id}] {context_metadata['source']}, \"\n",
        "      f\"page {context_metadata['page']}, \"\n",
        "      f\"start location {context_metadata['start_index']}\")\n",
        "\n",
        "  print(context_text)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "iG0P44Yanm2K",
        "outputId": "c7a32704-7054-4bd1-ae3f-082d7250db47"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is Transformer?\n",
            "\n",
            "Response: Transformer is a model architecture that employs a residual connection and layer normalization. It consists of an encoder and a decoder, both composed of stacked self-attention and fully connected layers. The Transformer is used in various domains such as NLP and computer vision. [1, 2, 3]\n",
            "\n",
            "[1] data/NIPS-2017-attention-is-all-you-need-Paper.pdf, page 2, start location 0\n",
            "Figure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 10] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "\n",
            "[2] data/NIPS-2017-attention-is-all-you-need-Paper.pdf, page 4, start location 503\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[31, 2, 8].\n",
            "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "\n",
            "[3] data/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf, page 1, start location 0\n",
            "In this paper, we seek to expand the applicability of\n",
            "Transformer such that it can serve as a general-purpose\n",
            "backbone for computer vision, as it does for NLP and\n",
            "as CNNs do in vision. We observe that signiﬁcant chal-\n",
            "lenges in transferring its high performance in the language\n",
            "domain to the visual domain can be explained by differ-\n",
            "ences between the two modalities. One of these differ-\n",
            "ences involves scale. Unlike the word tokens that serve\n",
            "as the basic elements of processing in language Trans-\n",
            "formers, visual elements can vary substantially in scale, a\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qE5cW_o4Hnvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOh-yvp1R2ky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}