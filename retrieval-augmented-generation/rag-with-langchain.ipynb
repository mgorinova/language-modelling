{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgorinova/language-modelling/blob/main/retrieval-augmented-generation/rag-with-langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V51p_Ia0zhtM"
      },
      "source": [
        "#### Prelims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEQSYxcEDKj5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchainhub sentence_transformers\n",
        "\n",
        "!pip install pypdf pdfminer.six\n",
        "!pip install chromadb\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZgeJzgu2SFst"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmUk1Y6bzW1I"
      },
      "source": [
        "Text wrapping of output cells, so that content is more readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MaJI8LMEzRHe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ2cu9hFGTWW"
      },
      "source": [
        "## Overview\n",
        "The goal of this Retrieval-Augmented Generation (RAG) task is to outline a prototype of a pipeline where we can ask natural language questions about the information in one of more input documents. The output should help answer the user's question, with a citation of the section of the document where this information is.\n",
        "\n",
        "This notebook uses Python and LangChain to implement such a prototype as follows:\n",
        "1. Extraction: Text is extracted from the .pdf documents and split into cunks.\n",
        "2. Embedding: The chunks are passed through a LLM to create embeddings. The embeddings are stored in a vector database (VB).\n",
        "3. Query the VB: Questions from the user and embedded with the same language model. We retrieve the most similar (in terms of embeddings) chunks from the VB.\n",
        "4. Prompt: The text of the chunks is incorporated into a LLM prompt as a context. The prompt also asks the question provided by the user.\n",
        "5. Assamble response.\n",
        "\n",
        "## 1. Extraction\n",
        "\n",
        "Use LangChain's document loaders to load the pdf files provided. The code assumes that all pdf files to be processed are in the local folder `data/`.\n",
        "\n",
        "For this example, I included two papers: the 2017 NeurIPS paper \"Attention is All You Need\", and the 2021 ICCV \"Swin Transformer\" paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CVab0xkJChNa",
        "outputId": "279ada50-26b1-4b96-a183-16823d819d98"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.document_loaders.pdf import PyPDFLoader\n",
        "\n",
        "data_path = \"data/\"\n",
        "\n",
        "pages = []\n",
        "for file_name in os.listdir(data_path):\n",
        "  if not file_name.endswith(\".pdf\"):\n",
        "    continue\n",
        "\n",
        "  file_path = os.path.join(data_path, file_name)\n",
        "  loader = PyPDFLoader(file_path)\n",
        "  pages.extend(loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tsSvlIzfvLs"
      },
      "source": [
        "Split the text into chunks. Using appropriate separateros, shunk sizes and overlap size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "Ruc8YZH3OX2A",
        "outputId": "70238a3d-7ba3-4d7a-b8f0-01efbc045bc3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': 'data/swin-transformer.pdf', 'page': 0, 'start_index': 1842}\n",
            "and models are publicly available at https://github.\n",
            "com/microsoft/Swin-Transformer .\n",
            "1. Introduction\n",
            "Modeling in computer vision has long been dominated\n",
            "by convolutional neural networks (CNNs). Beginning with\n",
            "AlexNet [35] and its revolutionary performance on the\n",
            "ImageNet image classiﬁcation challenge, CNN architec-\n",
            "*Equal contribution.†Interns at MSRA.‡Contact person.\n",
            "Figure 1. (a) The proposed Swin Transformer builds hierarchical\n",
            "feature maps by merging image patches (shown in gray) in deeper\n",
            "layers and has linear computation complexity to input image size\n",
            "due to computation of self-attention only within each local win-\n",
            "dow (shown in red). It can thus serve as a general-purpose back-\n",
            "bone for both image classiﬁcation and dense recognition tasks.\n",
            "(b) In contrast, previous vision Transformers [19] produce fea-\n",
            "ture maps of a single low resolution and have quadratic compu-\n",
            "tation complexity to input image size due to computation of self-\n",
            "attention globally.\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100, add_start_index=True)\n",
        "\n",
        "splits = text_splitter.split_documents(pages)\n",
        "\n",
        "print(splits[2].metadata)\n",
        "print(splits[2].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pCXUoJ5Sgek"
      },
      "source": [
        "## 2. Embeddings\n",
        "\n",
        "Embed the chunks using an LLM (in this case using HuggingFace `sentence_transformers` embedding models; with the default model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "47NKKOU6GzR5",
        "outputId": "24272ccb-c082-45bb-ac01-6c356a32a4d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-M9N1N2kbeZ"
      },
      "source": [
        "## 3. Query the Vector Database\n",
        "\n",
        "Next, we assume a question from the user and query the VB based on it to retrieve the top 5 most similar chunks in terms of semantics.\n",
        "\n",
        "I prepared 3 questions for this task: one relating to Document 1 (Attention is All You Need), one relating to Document 2 (Swin), and one relating to both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "ckpK5W5FocB_",
        "outputId": "73158be4-efd4-465f-c9fe-bcb9e62c85d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'page': 1,\n",
              "  'source': 'data/attention-is-all-you-need.pdf',\n",
              "  'start_index': 3529},\n",
              " {'page': 1, 'source': 'data/swin-transformer.pdf', 'start_index': 0},\n",
              " {'page': 2, 'source': 'data/attention-is-all-you-need.pdf', 'start_index': 0},\n",
              " {'page': 8, 'source': 'data/swin-transformer.pdf', 'start_index': 5475},\n",
              " {'page': 1, 'source': 'data/swin-transformer.pdf', 'start_index': 1852}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#question = \"Can I use self-attention for natural language processing tasks?\"\n",
        "#question = \"Can I use self-attention for computer vision tasks?\"\n",
        "question = \"What is Transformer?\"\n",
        "\n",
        "retrieved_chunks = vectorstore.similarity_search(query=question, k=5)\n",
        "\n",
        "[doc.metadata for doc in retrieved_chunks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "NLU43eeJ6sGk",
        "outputId": "4833a7f3-35e1-4d9e-960b-f5b5e85f5fa0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Figure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 10] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n"
          ]
        }
      ],
      "source": [
        "print(retrieved_chunks[2].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y48pPodzx7l7"
      },
      "source": [
        "## 4. Prompt\n",
        "\n",
        "We create a prompt for a LLM chatbot (in this case ChatGPT 3.5). The prompt incorporates the retrieved contexts and asks the chatbot to answer the question based on those contexts, by also providing a citation --- which contexts were the most influential for the construction of the chatbot's answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qIp1ktyxLZwz",
        "outputId": "927c6a7d-846f-4962-b0bf-434f7a8cad15"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"insert_your_api_key_here\"\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nJrjvSWYJiNP",
        "outputId": "15bca058-7df6-4114-a708-16b2a05f32fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.schema.document import Document\n",
        "\n",
        "def format_prompt(question: str, documents: list[Document]) -> str:\n",
        "  \"\"\"Function to create the LLM prompt based on the question and list of\n",
        "  retrieved documents.\"\"\"\n",
        "\n",
        "  formatted_context = \"\"\n",
        "  for i, d in enumerate(documents):\n",
        "    formatted_context += (\n",
        "      f\"Context {i+1}, source file {d.metadata['source']}, \"\n",
        "      f\"page {d.metadata['page']}, \"\n",
        "      f\"location {d.metadata['start_index']}:\\n{d.page_content}\\n\")\n",
        "\n",
        "  return f\"\"\"You are an assistant for question-answering tasks on documents.\n",
        "Use the following pieces of retrieved context from several existing documents to\n",
        "answer the question. If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Do not ask me questions, simply answer this query. State the IDs of the 1 to 3\n",
        "contexts that provided the most significant information for answering the query.\n",
        "Do this at the end of your response, in brackets like so: [2,3]\n",
        "\n",
        "Question: {question}\n",
        "{formatted_context}\n",
        "Answer:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SB2hg0u0eUxZ",
        "outputId": "60661040-6de6-483c-d754-6dd57559d060"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "llm_input = format_prompt(documents=retrieved_chunks, question=question)\n",
        "response = llm.invoke(input=llm_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "d19Bx9D7njjF",
        "outputId": "f510365f-5c16-4e2b-bdc7-ecd68c7996f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer is a model architecture that uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. It is used in natural language processing (NLP) tasks and can also be applied to computer vision tasks as a general-purpose backbone. One of the challenges in applying Transformer to computer vision is the differences in scale and resolution between visual elements and word tokens. [1, 2, 5]\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e951VwTvBnfy"
      },
      "source": [
        "Extract citation IDs from the response.\n",
        "\n",
        "**Note**: this has not been adapted to work in cases where citations were not returned, or the IDs were invalid.\n",
        "\n",
        "Perhaps a better way of doing this would be to embed the response and display / cite the most similar to it chunks of text, instead of relying on the chatbot to tell us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "nLQzuZZsAP_W",
        "outputId": "0a71e59d-5029-4224-bc05-7fd5eb642c23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[1, 2, 5]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "citations = re.findall(r'\\[(([1-9]+, ?)*([1-9]+))\\].? ?$', response.content)[0][0]\n",
        "context_ids = [int(c) for c in re.split(r'( ,)|,', citations) if c is not None]\n",
        "context_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UCaGSigCALS"
      },
      "source": [
        "## 5. Assemble the response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Dxabe0RgHzFd",
        "outputId": "8913d767-82dc-474c-ed98-28c2870f2788"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "context_texts = []\n",
        "for context_id in context_ids:\n",
        "  context_texts.append(retrieved_chunks[context_id-1].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iG0P44Yanm2K",
        "outputId": "e1e0b76c-182c-453c-93e3-669d165e8380"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is Transformer?\n",
            "\n",
            "Response: Transformer is a model architecture that uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. It is used in natural language processing (NLP) tasks and can also be applied to computer vision tasks as a general-purpose backbone. One of the challenges in applying Transformer to computer vision is the differences in scale and resolution between visual elements and word tokens. [1, 2, 5]\n",
            "\n",
            "[1] data/attention-is-all-you-need.pdf, page 1, start location 3529\n",
            "of continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\n",
            "sequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\n",
            "[9], consuming the previously generated symbols as additional input when generating the next.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "respectively.\n",
            "3.1 Encoder and Decoder Stacks\n",
            "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
            "sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "2\n",
            "\n",
            "[2] data/swin-transformer.pdf, page 1, start location 0\n",
            "In this paper, we seek to expand the applicability of\n",
            "Transformer such that it can serve as a general-purpose\n",
            "backbone for computer vision, as it does for NLP and\n",
            "as CNNs do in vision. We observe that signiﬁcant chal-\n",
            "lenges in transferring its high performance in the language\n",
            "domain to the visual domain can be explained by differ-\n",
            "ences between the two modalities. One of these differ-\n",
            "ences involves scale. Unlike the word tokens that serve\n",
            "as the basic elements of processing in language Trans-\n",
            "formers, visual elements can vary substantially in scale, a\n",
            "problem that receives attention in tasks such as object de-\n",
            "tection [38, 49, 50]. In existing Transformer-based mod-\n",
            "els [58, 19], tokens are all of a ﬁxed scale, a property un-\n",
            "suitable for these vision applications. Another difference\n",
            "is the much higher resolution of pixels in images com-\n",
            "pared to words in passages of text. There exist many vi-\n",
            "sion tasks such as semantic segmentation that require dense\n",
            "\n",
            "[5] data/swin-transformer.pdf, page 1, start location 1852\n",
            "locally within non-overlapping windows that partition an\n",
            "image (outlined in red). The number of patches in each\n",
            "window is ﬁxed, and thus the complexity becomes linear\n",
            "to image size. These merits make Swin Transformer suit-\n",
            "able as a general-purpose backbone for various vision tasks,\n",
            "in contrast to previous Transformer based architectures [19]\n",
            "which produce feature maps of a single resolution and have\n",
            "quadratic complexity.\n",
            "A key design element of Swin Transformer is its shift\n",
            "of the window partition between consecutive self-attention\n",
            "layers, as illustrated in Figure 2. The shifted windows\n",
            "bridge the windows of the preceding layer, providing con-\n",
            "nections among them that signiﬁcantly enhance modeling\n",
            "power (see Table 4). This strategy is also efﬁcient in re-\n",
            "gards to real-world latency: all query patches within a win-\n",
            "dow share the same keyset1, which facilitates memory ac-\n",
            "cess in hardware. In contrast, earlier sliding window based\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Question: {question}\\n\")\n",
        "\n",
        "print(f\"Response: {response.content}\\n\")\n",
        "\n",
        "for context_id, context_text in zip(context_ids, context_texts):\n",
        "  context_metadata = retrieved_chunks[context_id-1].metadata\n",
        "  print(\n",
        "      f\"[{context_id}] {context_metadata['source']}, \"\n",
        "      f\"page {context_metadata['page']}, \"\n",
        "      f\"start location {context_metadata['start_index']}\")\n",
        "\n",
        "  print(context_text)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qE5cW_o4Hnvi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZOh-yvp1R2ky"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "V51p_Ia0zhtM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
